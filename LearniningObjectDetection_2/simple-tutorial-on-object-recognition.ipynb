{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c905ffa7d7ebca3f49a439df1470a0b7620ee5d"
   },
   "source": [
    "# Simple Tutorial on Object Recognition 4 Beginner's!\n",
    "![](https://i0.wp.com/deepomatic.com/wp-content/uploads/2019/05/85384-1z4kp_opo_wiju9srtlv_1a.png?resize=1000%2C297&ssl=1)\n",
    "There is no real difference between object recognition and image recognition. In fact, they both refer to technologies that can recognize certain targeted subjects through specific algorithms like deep learning. They are strictly related to computer vision, which we define as the art and science of making computers understand images.\n",
    "\n",
    "In this kernel, we will go thru some basic concepts in object recognition: \n",
    "\n",
    "#### What is Object/Image recognition ?\n",
    "\n",
    "Object recognition consists of recognizing, identifying, and locating objects within a picture with a given degree of confidence.\n",
    "\n",
    "In this process, the four main tasks are:\n",
    "\n",
    "* Classification.\n",
    "* Tagging.\n",
    "* Detection.\n",
    "* Segmentation.\n",
    "\n",
    "#### Classification and tagging\n",
    "\n",
    "An important task in Object recognition is to identify what is in the image and with what level of confidence. \n",
    "![](https://i0.wp.com/deepomatic.com/wp-content/uploads/2019/05/85384-1z4kp_opo_wiju9srtlv_1a.png?resize=1000%2C297&ssl=1)\n",
    "\n",
    "The mechanism of this task is straightforward. It starts with the definition of the ontology, i.e. the class of objects to detect. Then, both classification and tagging identify what is in the image and the associated level of confidence. \n",
    "\n",
    "While **classification** recognizes only one class of objects, **tagging** can recognize multiple ones for a given image.\n",
    "\n",
    "In **classification** the algorithm will only remember that there is a dog, ignoring all other classes. \n",
    "\n",
    "In **tagging**, it will try to return all the best classes corresponding to the image. \n",
    "\n",
    "#### Detection and segmentation\n",
    "\n",
    "Once identified what is in the image, we want to locate the objects. There are two ways to do so: detection and segmentation.\n",
    "![](https://i0.wp.com/deepomatic.com/wp-content/uploads/2019/05/c74a7-1he3sybzm-ihofw4r39zgra.png?resize=1000%2C297&ssl=1)\n",
    "Detection outputs a rectangle, also called bounding box, where the objects are. It is a very robust technology, prone to minor errors and imprecisions. \n",
    "\n",
    "Segmentation identifies the objects for each pixelâ€Š in the image, resulting in a very precise map. However, the accuracy of segmentation depends on an extensive and often time-consuming training of the neural network.\n",
    "\n",
    "This kernel covers the Object Recognition using CNN, Keras ,Tensorflowworks implementation using image dataset . The following topics will be covered.\n",
    "\n",
    "### Table Of Contents\n",
    "\n",
    "#### 1. Loading and preprocess Object dataset\n",
    "#### 2. Designing and training a CNN model in Keras\n",
    "#### 3. Plotting the Loss and Accuracy  curve\n",
    "#### 4. Evaluating the model & Predicting the output class of a test Object\n",
    "#### 5. Visualizing the intermediate layer output of CNN\n",
    "#### 6. Plotting the confusion matrix for your result\n",
    "\n",
    "### 1. Loading and preprocessing own dataset\n",
    "\n",
    "The dataset that I am using for this kernel is my own accumulated dataset of 7 types of classes namely \n",
    "'flowers', 'cars', 'cats', 'horses', 'human', 'bike', 'dogs' with total of 1803 image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "50d8610b21e1cdfa2be644a87ac23a5635bdbf85",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:21:22.807190Z",
     "iopub.status.busy": "2021-09-11T13:21:22.806804Z",
     "iopub.status.idle": "2021-09-11T13:21:22.816639Z",
     "shell.execute_reply": "2021-09-11T13:21:22.815332Z",
     "shell.execute_reply.started": "2021-09-11T13:21:22.807136Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Import Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "# Import tensorflow as the backend for Keras\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD,RMSprop\n",
    "from keras.callbacks import TensorBoard\n",
    "# Import required libraries for cnfusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e5df0fa81a8a229ad7d6f681fd0ac9fe74368166",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:21:26.396438Z",
     "iopub.status.busy": "2021-09-11T13:21:26.396067Z",
     "iopub.status.idle": "2021-09-11T13:21:26.430206Z",
     "shell.execute_reply": "2021-09-11T13:21:26.428918Z",
     "shell.execute_reply.started": "2021-09-11T13:21:26.396384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bike', 'cars', 'cats', 'dogs', 'flowers', 'horses', 'human']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "# Define data path\n",
    "data_path = '../../datasets/objects2/data'\n",
    "data_dir_list = os.listdir(data_path)\n",
    "data_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2edb741147fd4be34950b628258c48e96cf2259b",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:27:47.313008Z",
     "iopub.status.busy": "2021-09-11T13:27:47.312583Z",
     "iopub.status.idle": "2021-09-11T13:27:52.206157Z",
     "shell.execute_reply": "2021-09-11T13:27:52.205382Z",
     "shell.execute_reply.started": "2021-09-11T13:27:47.312916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images of dataset-bike\n",
      "\n",
      "Loaded the images of dataset-cars\n",
      "\n",
      "Loaded the images of dataset-cats\n",
      "\n",
      "Loaded the images of dataset-dogs\n",
      "\n",
      "Loaded the images of dataset-flowers\n",
      "\n",
      "Loaded the images of dataset-horses\n",
      "\n",
      "Loaded the images of dataset-human\n",
      "\n",
      "(1803, 128, 128, 3)\n",
      "(1803, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "img_rows=128\n",
    "img_cols=128\n",
    "num_channel=1\n",
    "num_epoch=100\n",
    "# Define the number of classes\n",
    "num_classes = 7\n",
    "img_data_list=[]\n",
    "for dataset in data_dir_list:\n",
    "\timg_list=os.listdir(data_path+'/'+ dataset)\n",
    "\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "\tfor img in img_list:\n",
    "\t\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "\t\t#input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "\t\tinput_img_resize=cv2.resize(input_img,(128,128))\n",
    "\t\timg_data_list.append(input_img_resize)\n",
    "\n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print (img_data.shape)\n",
    "print (img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T13:28:03.504878Z",
     "iopub.status.busy": "2021-09-11T13:28:03.504187Z",
     "iopub.status.idle": "2021-09-11T13:28:03.512259Z",
     "shell.execute_reply": "2021-09-11T13:28:03.511024Z",
     "shell.execute_reply.started": "2021-09-11T13:28:03.504775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1803, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print (img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1c7bbbe0552a6fafbbddd8b52c0a869c30846863",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:28:14.400093Z",
     "iopub.status.busy": "2021-09-11T13:28:14.399763Z",
     "iopub.status.idle": "2021-09-11T13:28:14.410607Z",
     "shell.execute_reply": "2021-09-11T13:28:14.409173Z",
     "shell.execute_reply.started": "2021-09-11T13:28:14.400033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(1803, 128, 128, 3)\n",
      "(1803, 1, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(num_channel)\n",
    "print (img_data.shape)\n",
    "\n",
    "if num_channel==1:\n",
    "\tif K.image_data_format()=='channels_first':\n",
    "\t\timg_data= np.expand_dims(img_data, axis=1) \n",
    "\t\tprint (img_data.shape)\n",
    "\telse:\n",
    "\t\tprint (img_data.shape)\n",
    "\t\timg_data= np.expand_dims(img_data, axis=3) \n",
    "\t\tprint (img_data.shape)\n",
    "\t\t\n",
    "else:\n",
    "\tif K.image_data_format()=='channels_first':\n",
    "\t\timg_data=np.rollaxis(img_data,3,1)\n",
    "\t\tprint (img_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4e59f47240cedd2e413aef6600c2d51f4770eaa"
   },
   "source": [
    "Assigning Labels & define the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a624f7953cc5608fbbaaca46c946fede0fbc6397",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:31:44.712122Z",
     "iopub.status.busy": "2021-09-11T13:31:44.711713Z",
     "iopub.status.idle": "2021-09-11T13:31:44.721243Z",
     "shell.execute_reply": "2021-09-11T13:31:44.719856Z",
     "shell.execute_reply.started": "2021-09-11T13:31:44.712061Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "num_of_samples = img_data.shape[0]\n",
    "labels = np.ones((num_of_samples,),dtype='int64')\n",
    "labels[0:365]=0\n",
    "labels[365:567]=1\n",
    "labels[567:987]=2\n",
    "labels[987:1189]=3\n",
    "labels[1189:1399]=4\n",
    "labels[1399:1601]=5\n",
    "labels[1601:1803]=6\n",
    "names = ['bike', 'cars', 'cats', 'dogs', 'flowers', 'horses', 'human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8aec9a375391590b237e6d65c7d15a32d0dcddf"
   },
   "source": [
    "Convert class labels to on-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0a9feefa3b02324961a58ebf7d53068ec1782f8f",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:32:00.802994Z",
     "iopub.status.busy": "2021-09-11T13:32:00.802582Z",
     "iopub.status.idle": "2021-09-11T13:32:00.809742Z",
     "shell.execute_reply": "2021-09-11T13:32:00.807649Z",
     "shell.execute_reply.started": "2021-09-11T13:32:00.802910Z"
    }
   },
   "outputs": [],
   "source": [
    "Y = np_utils.to_categorical(labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7abeb1d6a14c6e3bc5761cf7a0820c1db149da0a"
   },
   "source": [
    "Shuffle and Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6da54e4b62d7bc9116a252804b9a5b8adc521066",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:32:02.223033Z",
     "iopub.status.busy": "2021-09-11T13:32:02.222459Z",
     "iopub.status.idle": "2021-09-11T13:32:02.460499Z",
     "shell.execute_reply": "2021-09-11T13:32:02.459316Z",
     "shell.execute_reply.started": "2021-09-11T13:32:02.222914Z"
    }
   },
   "outputs": [],
   "source": [
    "x,y = shuffle(img_data,Y, random_state=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5a82cb80d1b892aa47c66598d685ce85bd749a37",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:32:17.913063Z",
     "iopub.status.busy": "2021-09-11T13:32:17.912665Z",
     "iopub.status.idle": "2021-09-11T13:32:17.918506Z",
     "shell.execute_reply": "2021-09-11T13:32:17.917660Z",
     "shell.execute_reply.started": "2021-09-11T13:32:17.913000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (1442, 1, 128, 128, 3)\n",
      "X_test shape = (361, 1, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape = {}\".format(X_train.shape))\n",
    "print(\"X_test shape = {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "44a970871ca0c1a83c14fe0d5753221f1474cd28",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:32:27.011039Z",
     "iopub.status.busy": "2021-09-11T13:32:27.010428Z",
     "iopub.status.idle": "2021-09-11T13:32:27.263995Z",
     "shell.execute_reply": "2021-09-11T13:32:27.262652Z",
     "shell.execute_reply.started": "2021-09-11T13:32:27.010967Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 49152 into shape (128,128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_119651/2136339697.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1203\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 49152 into shape (128,128)"
     ]
    }
   ],
   "source": [
    "image = X_train[1203,:].reshape((128,128))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ffe8194fd6aa172549d158ee67a9d26f7b1786a0"
   },
   "source": [
    "### 2. Designing and training a CNN model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4e3056be0e953f3009fe460f578eedfa9329809f",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:33:00.232339Z",
     "iopub.status.busy": "2021-09-11T13:33:00.231795Z",
     "iopub.status.idle": "2021-09-11T13:33:00.465895Z",
     "shell.execute_reply": "2021-09-11T13:33:00.465129Z",
     "shell.execute_reply.started": "2021-09-11T13:33:00.232284Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initialising the input shape\n",
    "input_shape=img_data[0].shape\n",
    "# Design the CNN Sequential model\n",
    "cnn_model = Sequential([\n",
    "    Convolution2D(32,3,3,padding ='same',activation='relu',input_shape = input_shape),\n",
    "    Convolution2D(32,3,3,activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)) ,\n",
    "    Dropout(0.5),\n",
    "    Flatten(), \n",
    "    Dense(128,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes,activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1802f901ff9ae206dfa0a891a4aa5a1db766f0e"
   },
   "source": [
    "**Compiling the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0c0bdef6759454163e3e43533ca21e373fc03751",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:33:02.751462Z",
     "iopub.status.busy": "2021-09-11T13:33:02.751005Z",
     "iopub.status.idle": "2021-09-11T13:33:02.802151Z",
     "shell.execute_reply": "2021-09-11T13:33:02.800984Z",
     "shell.execute_reply.started": "2021-09-11T13:33:02.751382Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adadelta',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5e069f02a9b6eee4f57f91b01d074d72ab1bcb"
   },
   "source": [
    "### View Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c030620eee20f5e588ab79190cf106d1d1e87564",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:33:06.682858Z",
     "iopub.status.busy": "2021-09-11T13:33:06.682535Z",
     "iopub.status.idle": "2021-09-11T13:33:06.691326Z",
     "shell.execute_reply": "2021-09-11T13:33:06.690179Z",
     "shell.execute_reply.started": "2021-09-11T13:33:06.682808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 32, 43, 43)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 14, 14)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 7, 7)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 7, 7)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               200832    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 211,303\n",
      "Trainable params: 211,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e812bdbd4317c8caa2a63f9aa27fe21c14d02d9"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "93f00a8f4ed25ecb6facd31aa75c29677e386362",
    "execution": {
     "iopub.execute_input": "2021-09-11T13:33:43.400275Z",
     "iopub.status.busy": "2021-09-11T13:33:43.399819Z",
     "iopub.status.idle": "2021-09-11T13:36:48.604191Z",
     "shell.execute_reply": "2021-09-11T13:36:48.602082Z",
     "shell.execute_reply.started": "2021-09-11T13:33:43.400206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1442 samples, validate on 361 samples\n",
      "Epoch 1/100\n",
      "1442/1442 [==============================] - 63s 43ms/step - loss: 2.0128 - acc: 0.2469 - val_loss: 2.5172 - val_acc: 0.1330\n",
      "Epoch 2/100\n",
      "1442/1442 [==============================] - 61s 42ms/step - loss: 1.7055 - acc: 0.3467 - val_loss: 1.5978 - val_acc: 0.3906\n",
      "Epoch 3/100\n",
      " 944/1442 [==================>...........] - ETA: 31s - loss: 1.5419 - acc: 0.4269"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cb226c9ea28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = cnn_model.fit(X_train, y_train, batch_size=16, nb_epoch=num_epoch, verbose=1, validation_data=(X_test, y_test))\n",
    "train_loss=hist.history['loss']\n",
    "val_loss=hist.history['val_loss']\n",
    "train_acc=hist.history['acc']\n",
    "val_acc=hist.history['val_acc']\n",
    "xc=range(num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6242371a4823e8bdc158edca90e24bdc976cd49"
   },
   "source": [
    "### 3. Plotting the Loss and Accuracy curve\n",
    "#### Visualizing Training Loss & Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "38913035e744610f46eac8792b6b4fa5c5f497b7"
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,5))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,val_loss)\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train Loss vs Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.style.use(['classic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "745a043a3306873626bb70baa3e137914ad9f63e"
   },
   "source": [
    "#### Visualizing Training Accuracy & Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7b358a5ef10a5a8d9576edf03c1f3f71042287d7"
   },
   "outputs": [],
   "source": [
    "plt.figure(2,figsize=(10,5))\n",
    "plt.plot(xc,train_acc)\n",
    "plt.plot(xc,val_acc)\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train Accuracy vs Validation Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend(['Train Accuracy','Validation Accuracy'],loc=4)\n",
    "plt.style.use(['classic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "268ef64abed971d8538ad2fccac1bfa190288db9"
   },
   "source": [
    "### 4. Evaluating the model & Predicting the output class of a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "08cb962cd44da23135f5b556f8cbbd265741fdd7"
   },
   "outputs": [],
   "source": [
    "score = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88b9fc142a9ec87bf22a24f94dbd0329381b2670"
   },
   "source": [
    "Now let use a test image and predict the probability of this image belonging to which class.Let us find out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6c7110c7d4393cb1091ec31b54eec71205da19e0"
   },
   "outputs": [],
   "source": [
    "test_image = X_test[0:1]\n",
    "print (test_image.shape)\n",
    "print(cnn_model.predict(test_image))\n",
    "print(cnn_model.predict_classes(test_image))\n",
    "print(y_test[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5533656be494eeaf2a0181a31bad8efddd63e4f1"
   },
   "source": [
    "So based on the above results it is evident that predict class is 1 .In this case it is a car. Let us visualise it ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b0ff16f2733d602f19a2765ac0a67061c88c5cd2"
   },
   "outputs": [],
   "source": [
    "image = test_image.reshape((128,128))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e10ebf4b105d7143c4e0dfddc51cd79f5327d6cb"
   },
   "source": [
    "### Test with a new image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "bcda3db8a01cb0cd0136423d1d7453a34178f723"
   },
   "outputs": [],
   "source": [
    "test_img = cv2.imread('../input/data/data/human/rider-104.jpg')\n",
    "test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)\n",
    "test_img = cv2.resize(test_img,(128,128))\n",
    "test_img = np.array(test_img)\n",
    "test_img = test_img.astype('float32')\n",
    "test_img /= 255\n",
    "print(test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f60222887e15bde2d0dab7c243156a368aa89b0e"
   },
   "outputs": [],
   "source": [
    "image = test_img.reshape((128,128))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "294f05ffbf783f16401390520cf064645cb16803"
   },
   "outputs": [],
   "source": [
    "if num_channel==1:\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\ttest_img= np.expand_dims(test_img, axis=0)\n",
    "\t\ttest_img= np.expand_dims(test_img, axis=0)\n",
    "\t\tprint (test_img.shape)\n",
    "\telse:\n",
    "\t\ttest_img= np.expand_dims(test_img, axis=3) \n",
    "\t\ttest_img= np.expand_dims(test_img, axis=0)\n",
    "\t\tprint (test_img.shape)\n",
    "\t\t\n",
    "else:\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\ttest_img=np.rollaxis(test_img,2,0)\n",
    "\t\ttest_img= np.expand_dims(test_img, axis=0)\n",
    "\t\tprint (test_img.shape)\n",
    "\telse:\n",
    "\t\ttest_img= np.expand_dims(test_img, axis=0)\n",
    "\t\tprint (test_img.shape)\n",
    "\t\t\n",
    "# Predicting the test image\n",
    "print((cnn_model.predict(test_img)))\n",
    "print(cnn_model.predict_classes(test_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54cb565519a3231351663193c8cccfa690c55cf8"
   },
   "source": [
    "So from the above prediction it is evident that the model predict the class as human for the test image that we have picked up .\n",
    "\n",
    "### 5. Visualizing the intermediate layer output of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2aefbc512f09c29250213ba5396ff58c6c026368"
   },
   "outputs": [],
   "source": [
    "def get_featuremaps(cnn_model, layer_idx, X_batch):\n",
    "\tget_activations = K.function([cnn_model.layers[0].input, K.learning_phase()],[cnn_model.layers[layer_idx].output,])\n",
    "\tactivations = get_activations([X_batch,0])\n",
    "\treturn activations\n",
    "layer_num=3\n",
    "filter_num=0\n",
    "activations = get_featuremaps(cnn_model, int(layer_num),test_img)\n",
    "print (np.shape(activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c4bffe62b5b632722dd51470ec9374df1140844a"
   },
   "outputs": [],
   "source": [
    "feature_maps = activations[0][0]      \n",
    "print (np.shape(feature_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9a9323c913752574d4aeac78575ec56b5c3c8672"
   },
   "outputs": [],
   "source": [
    "if K.image_dim_ordering()=='th':\n",
    "\tfeature_maps=np.rollaxis((np.rollaxis(feature_maps,2,0)),2,0)\n",
    "print (feature_maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ae76cd9da7da9abe874e18399a0c31123e698d7c"
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16,16))\n",
    "plt.imshow(feature_maps[:,:,filter_num],cmap='gray')\n",
    "plt.savefig(\"featuremaps-layer-{}\".format(layer_num) + \"-filternum-{}\".format(filter_num)+'.jpg')\n",
    "num_of_featuremaps=feature_maps.shape[2]\n",
    "fig=plt.figure(figsize=(16,16))\t\n",
    "plt.title(\"featuremaps-layer-{}\".format(layer_num))\n",
    "subplot_num=int(np.ceil(np.sqrt(num_of_featuremaps)))\n",
    "for i in range(int(num_of_featuremaps)):\n",
    "\tax = fig.add_subplot(subplot_num, subplot_num, i+1)\n",
    "\t#ax.imshow(output_image[0,:,:,i],interpolation='nearest' ) #to see the first filter\n",
    "\tax.imshow(feature_maps[:,:,i],cmap='gray')\n",
    "\tplt.xticks([])\n",
    "\tplt.yticks([])\n",
    "\tplt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f92a7b9ac3305869c3b551cff4c790e0421de1b"
   },
   "source": [
    "### 6. Plotting the confusion matrix to observe the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c249ddf78fbb817f73b39e53467f5babeee16a8b"
   },
   "outputs": [],
   "source": [
    "# Print the confusion matrix\n",
    "Y_pred = cnn_model.predict(X_test)\n",
    "print(Y_pred)\n",
    "y_pred = np.argmax(Y_pred,axis=1)\n",
    "print(y_pred)\n",
    "target_names=['Class 0 (flowers)', 'Class 1 (cars)', 'Class 2 (cats)', 'Class 3 (horses)',\n",
    "              'Class 4 (human)', 'Class 5 (bike)', 'Class 6 (dogs)']\n",
    "print(classification_report(np.argmax(y_test,axis=1),y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22a40bf6f8ab7695d7c096c33d1b93659193c2a0"
   },
   "source": [
    "As shown above in the classification report the recall shows the individual class accuracy.In this case horse and dogs have a very low class accuracy of 0.38 and 0.32 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a3477142f66903264c28d8bc2e009ee015869625"
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6cc2de0864a5fa160dd8a8fb42f60179ed937924"
   },
   "source": [
    "From the above confusion matrix each row represents a class .For example let us look at the first row where it shows 37 images are classified for class 0 (flowers) and rest across other classes. To visualise this in a more understandable format let us plot it using matplotlib library as below \n",
    "\n",
    "**Plotting the confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "498aa271e21157fe106b826be4d14885810e3299"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float32') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Confusion matrix with Normalization\")\n",
    "    else:\n",
    "        print('Confusion matrix without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "117a241c7e783a693b9a9ea140d1f0aa7af92f08"
   },
   "source": [
    "### Plot non-normalized confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "551103f79aab0415d94a28bd9f48f6962251ca49"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion Matrix without Normalisation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "abd45e6f9dd8172db9c89804f3e976772d92b26a"
   },
   "source": [
    "Now let us plot confusion matrix with normalization which means all values will lie between 0 and 1.\n",
    "**Plot normalized confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4d9618bb86818eb963b32f909e1abae96b574623"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True,\n",
    "                      title='Normalized Confusion Matrix')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df3e817a4a61742fd9f6e320c53a7b34bb9f5266"
   },
   "source": [
    "## I hope by now you are able to completely understand how to do Object Recognition using deep learning CNN model.\n",
    "\n",
    "# Please do leave your comments/suggestions and you like this kernel greatly appreciate to UPVOTE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
